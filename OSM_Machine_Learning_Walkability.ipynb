{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Import the Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from _settings import * #this has some sensitive stuff like api keys and usernames for carto\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create the data folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pathList = ['input','output','processing','machine_learning','machine_learning/output']\n",
    "\n",
    "dataDir = 'data_v2/'\n",
    "\n",
    "def makeDirectory(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "for i in pathList:\n",
    "    makeDirectory(dataDir + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielmsheehan/GitHub/osm_walkability\n"
     ]
    }
   ],
   "source": [
    "SAVE_FOLDER = dataDir\n",
    "INPUT_FOLDER = SAVE_FOLDER + 'input/'\n",
    "PROCESSING_FOLDER = SAVE_FOLDER + 'processing/'\n",
    "ML_FOLDER = SAVE_FOLDER + 'machine_learning/'\n",
    "OUTPUT_FOLDER = SAVE_FOLDER + 'output/'\n",
    "curDir = os.getcwd()\n",
    "print curDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stop here if just trying to import modules and set the directories. \n",
    "\n",
    "## Download the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data_v2/input/beh_nyc_walkability.csv',\n",
       " <httplib.HTTPMessage instance at 0x10ee5ddd0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://dms2203.cartodb.com/api/v2/sql?filename=beh_walkability_ct2010.csv&format=csv&q=SELECT geoid,t10km2,t10lndkm2,t10cnt,t10resdn1,t10intden,t10entrpy,t10rtlfar,t10sub07d,t10walk,t10walkc,the_geom FROM ct10'\n",
    "\n",
    "dl_file = urllib.URLopener()\n",
    "dl_file.retrieve(url, INPUT_FOLDER + 'beh_nyc_walkability.csv')\n",
    "\n",
    "url = 'https://s3.amazonaws.com/metro-extracts.mapzen.com/new-york_new-york.osm.pbf'\n",
    "\n",
    "dl_file = urllib.URLopener()\n",
    "dl_file.retrieve(url, INPUT_FOLDER + 'new-york_new-york.osm.pbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paste GDAL/OGR commands into terminal\n",
    "Currently not working using os.system in this virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogr2ogr -f \"SQLite\" -dsco SPATIALITE=YES data_v2/input/new-york_new-york.db data_v2/input/new-york_new-york.osm.pbf\n"
     ]
    }
   ],
   "source": [
    "thecmd = 'ogr2ogr -f \"SQLite\" -dsco SPATIALITE=YES ' + dataDir + 'input/new-york_new-york.db ' + dataDir + 'input/new-york_new-york.osm.pbf'\n",
    "print thecmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogr2ogr -f \"CSV\" data_v2/input/new-york_new-york_points.csv data_v2/input/new-york_new-york.db -lco GEOMETRY=AS_XY -progress -explodecollections -sql \"select * from points\"\n"
     ]
    }
   ],
   "source": [
    "thecmd = 'ogr2ogr -f \"CSV\" ' + dataDir + 'input/new-york_new-york_points.csv ' + dataDir + 'input/new-york_new-york.db -lco GEOMETRY=AS_XY -progress -explodecollections -sql \"select * from points\"'\n",
    "print thecmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean up the OSM data\n",
    "So its just **osm_id** and **latitude, longitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inCSV = dataDir+'input/new-york_new-york_points.csv'\n",
    "df = pd.read_csv(inCSV)\n",
    "df = df[['X','Y','osm_id']]\n",
    "df.columns = ['longitude','latitude','osm_id'] #just delcare col names here\n",
    "df.to_csv(dataDir+'processing/new-york_new-york_points.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Upload OSM point data to Carto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'processing/new-york_new-york_points.csv \"https://'+USERNAME+'.carto.com/api/v1/imports/?api_key=\"'+APIKEY\n",
    "os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the BEH Walkability data to Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'input/beh_nyc_walkability.csv \"https://'+USERNAME+'.carto.com/api/v1/imports/?api_key=\"'+APIKEY\n",
    "os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run an intersection to via the Carto Batch SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))'\n",
    "thecmd = '''curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "  \"query\": \"'''+query+'''\"\n",
    "}' \"http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batcmd = thecmd\n",
    "result = subprocess.check_output(batcmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user\":\"dms2203\",\"query\":\"SELECT cdb_cartodbfytable(new_york_new_york_points_int_ct10_v2)\",\"job_id\":\"43d3b877-3ba0-4273-829b-beb1cf5af9a3\",\"created_at\":\"2017-09-07T21:22:59.866Z\",\"updated_at\":\"2017-09-07T21:22:59.866Z\",\"status\":\"pending\"}\n",
      "curl -X POST -H \"Content-Type: application/json\" -d '{\n",
      "  \"query\": \"SELECT cdb_cartodbfytable('new_york_new_york_points_int_ct10_v2')\"\n",
      "}' \"http://dms2203.carto.com/api/v2/sql/job?api_key=eb4e90135e42f41bda36e58b7e9c1a660acd98d1\"\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CartoDBfy the resulting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT cdb_cartodbfytable('new_york_new_york_points_int_ct10')\"\n",
    "thecmd = '''curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "  \"query\": \"'''+query+'''\"\n",
    "}' \"http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batcmd = thecmd\n",
    "result = subprocess.check_output(batcmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user\":\"dms2203\",\"query\":\"SELECT cdb_cartodbfytable(new_york_new_york_points_int_ct10)\",\"job_id\":\"d2871209-b5f8-4f1c-9e2a-cb05a737db9d\",\"created_at\":\"2017-09-11T15:27:00.351Z\",\"updated_at\":\"2017-09-11T15:27:00.351Z\",\"status\":\"pending\"}\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or run your SQL via the [Carto Batch API](https://cartodb.github.io/customer_success/batch/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download the OSM - BEH Walkability Census Tracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data_v2/processing//new_york_new_york_points_int_ct10.csv',\n",
       " <httplib.HTTPMessage instance at 0x114b5d4d0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://dms2203.cartodb.com/api/v2/sql?format=csv&q=SELECT geoid, osm_id, latitude, longitude FROM new_york_new_york_points_int_ct10&api_key='+APIKEY\n",
    "\n",
    "dl_file = urllib.URLopener()\n",
    "dl_file.retrieve(url, PROCESSING_FOLDER+'/new_york_new_york_points_int_ct10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split up OSM Highway & Tags Other into seperate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inCSV = INPUT_FOLDER+'/new-york_new-york_points.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "dfH = df[['osm_id','highway']]\n",
    "dfT = df[['osm_id','other_tags']]\n",
    "\n",
    "dfH = dfH.dropna(subset=['highway'])\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/osm_highway.csv'\n",
    "dfH.to_csv(ouCSV, index=False)\n",
    "\n",
    "dfT = dfT.dropna(subset=['other_tags'])\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/osm_other_tags.csv'\n",
    "dfT.to_csv(ouCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Highway Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "inCSV = PROCESSING_FOLDER+'/osm_highway.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "df['count'] = 1\n",
    "\n",
    "dfp = pd.pivot_table(df, values='count', index='osm_id',columns='highway', aggfunc=np.sum)\n",
    "\n",
    "dfp = dfp.rename(columns=lambda x: x.replace(';', '_'))\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/osm_highway_pivot.csv'\n",
    "\n",
    "dfp.to_csv(ouCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parse Tags Other \n",
    "This took a very long long time to run. I should look into rewriting the code, maybe not store all these dataframes into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...concatenating...\n"
     ]
    }
   ],
   "source": [
    "inCSV = PROCESSING_FOLDER + '/osm_other_tags.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "tagList = df.values.tolist()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for h in tagList:\n",
    "    bucket = []\n",
    "    x = h[1].split('\",\"')\n",
    "    \n",
    "    for i in x:\n",
    "        z = i.split(\"=>\")\n",
    "        osm = []\n",
    "        \n",
    "        for j in z: \n",
    "            osm.append(j.replace('\"','').replace(':','_'))\n",
    "        bucket.append(osm)\n",
    "\n",
    "    df = pd.DataFrame(data=[h[0]], columns=['osm_id'])\n",
    "    \n",
    "    for i in bucket:\n",
    "        df[i[0]] = i[1]\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "print '...concatenating...'\n",
    "df = pd.concat(df_list)\n",
    "df = df.set_index('osm_id')\n",
    "\n",
    "df.to_csv(PROCESSING_FOLDER + '/osm_tags_pivot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean up Other Tags reduce low features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addr_city 0.968342437816 363\n",
      "addr_state 0.971963337785 16\n",
      "amenity 0.880495836688 122\n",
      "capacity 0.975261201406 88\n",
      "cityracks.large 0.977893706577 9\n",
      "cityracks.small 0.977893706577 9\n",
      "crossing 0.974528342824 10\n",
      "cuisine 0.985419971361 374\n",
      "gnis_county_id 0.958183667861 29\n",
      "gnis_created 0.956901165342 169\n",
      "gnis_state_id 0.958183667861 3\n",
      "leisure 0.988780513676 35\n",
      "natural 0.966071540498 15\n",
      "operator 0.988370691442 404\n",
      "power 0.953612944597 10\n",
      "railway 0.968294223435 21\n",
      "religion 0.986027472554 9\n",
      "shop 0.977127097928 183\n"
     ]
    }
   ],
   "source": [
    "inCSV = PROCESSING_FOLDER + '/osm_tags_pivot.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "dfLen = len(df.index)\n",
    "\n",
    "keeperCols = []\n",
    "\n",
    "for i in df.columns:\n",
    "    countNulls = df[i].isnull().sum()\n",
    "    pctNull = countNulls/(dfLen * 1.0)\n",
    "    if df[i].nunique() < 500:\n",
    "        if pctNull < 0.99:\n",
    "            print i, pctNull, df[i].nunique()\n",
    "            keeperCols.append(i)\n",
    "\n",
    "keeperCols     \n",
    "    \n",
    "df = df[keeperCols+['osm_id']]        \n",
    "\n",
    "ouCSV = PROCESSING_FOLDER + '/osm_tags_pivot_cln.csv'\n",
    "\n",
    "df.to_csv(ouCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merge data, One Hot Encode, Normalize variables to Count by Square Kilometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inTra = INPUT_FOLDER + '/beh_nyc_walkability.csv'\n",
    "inInt = PROCESSING_FOLDER + '/new_york_new_york_points_int_ct10.csv'\n",
    "inHig = PROCESSING_FOLDER + '/osm_highway_pivot.csv'\n",
    "inTag = PROCESSING_FOLDER + '/osm_tags_pivot_cln.csv'\n",
    "\n",
    "dfInt = pd.read_csv(inInt)\n",
    "dfTra = pd.read_csv(inTra)\n",
    "dfHig = pd.read_csv(inHig)\n",
    "dfTag = pd.read_csv(inTag)\n",
    "\n",
    "dfInt = dfInt[['geoid','osm_id']]\n",
    "dfTra = dfTra[['geoid','t10walk','t10lndkm2']]\n",
    "\n",
    "df = dfInt.merge(dfTra, how='left', on='geoid')\n",
    "df = df.merge(dfHig, how='left', on='osm_id')\n",
    "df = df.merge(dfTag, how='left', on='osm_id')\n",
    "\n",
    "df = df.drop(['osm_id','gnis_created','gnis_county_id','gnis_state_id','capacity','addr_state','addr_city','religion'], axis=1)\n",
    "#df = df.drop(['osm_id'], axis=1)\n",
    "\n",
    "dfStringCols = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object': #select all non numerical columns\n",
    "        dfStringCols.append(i)\n",
    "\n",
    "df['index_col'] = df.index\n",
    "\n",
    "colTotal = 0\n",
    "for i in dfStringCols:\n",
    "    one_hot = pd.get_dummies(df[i])\n",
    "    colTotal = colTotal + len(one_hot.columns)\n",
    "    one_hot = one_hot.rename(columns = lambda x : i + '_one_hot_' + x)\n",
    "    one_hot['index_col'] = one_hot.index\n",
    "    df = df.merge(one_hot, on='index_col', how='left')\n",
    "\n",
    "df = df.drop(dfStringCols, axis=1)\n",
    "df = df.groupby(['geoid','t10walk','t10lndkm2'],as_index=False).sum() # sum osm features to tract boundaries\n",
    "df = df.fillna(0)\n",
    "\n",
    "colsList = df.columns\n",
    "\n",
    "for i in colsList:\n",
    "    if i != 'geoid' and i != 'osm_id'  and i != 't10walk'  and i != 't10lndkm2':\n",
    "        df[i] = df[i]/df['t10lndkm2'] # get osm feature density by tract area\n",
    "\n",
    "df = df.drop(['t10lndkm2','index_col'], axis=1)\n",
    "df = df.fillna(0)\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/walkability_highway_tags_features.csv'\n",
    "df.to_csv(ouCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create Training and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2163\n",
      "1764\n",
      "399\n"
     ]
    }
   ],
   "source": [
    "inCSV = PROCESSING_FOLDER + '/walkability_highway_tags_features.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8 #split 80/20, 80% for training, 20% for testing - http://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "print len(df)\n",
    "print len(train)\n",
    "print len(test)\n",
    "\n",
    "test.to_csv(ML_FOLDER + '/test_w_label.csv', index=False)\n",
    "df.to_csv(ML_FOLDER + '/all_w_label.csv', index=False)\n",
    "\n",
    "testDropCols = ['t10walk'] #['health','label']\n",
    "\n",
    "test = test.drop(testDropCols,axis=1)\n",
    "df = df.drop(testDropCols,axis=1)\n",
    "\n",
    "train.to_csv(ML_FOLDER + '/train.csv',index=False)\n",
    "test.to_csv(ML_FOLDER + '/test.csv', index=False)\n",
    "df.to_csv(ML_FOLDER + '/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmsheehan/GitHub/osm_walkability/env/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-11 13:21:53.771493\n",
      "2017-09-11 13:21:54.002022\n",
      "training time: 0:00:00.033346\n",
      "training accuracy: 0.937870370514\n",
      "cross validating...\n",
      "[-0.2219347  -2.36244634 -0.01105474 -0.80244384 -0.63520043]\n",
      "2017-09-11-13-21-54-361501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "print datetime.now()\n",
    " \n",
    "wd = ML_FOLDER + '/'\n",
    " \n",
    "def main():\n",
    "    data = pd.read_csv(wd + 'train.csv').astype(np.float32)\n",
    "    data = data.replace(np.inf, 0)\n",
    "    data = data.drop('geoid',axis=1)\n",
    "    data = data.fillna(data.mean())\n",
    "    feature_names = data.columns\n",
    "    \n",
    "    t0 = datetime.now()\n",
    "\n",
    "    X = data[data.columns.difference(['t10walk'])]\n",
    "    Y = data['t10walk']\n",
    "\n",
    "    clf = tree.DecisionTreeRegressor()\n",
    "    #clf = ExtraTreeRegressor()\n",
    "\n",
    "    print datetime.now()\n",
    " \n",
    "    t0 = datetime.now()\n",
    "    clf.fit(X, Y)\n",
    "    print \"training time:\", (datetime.now() - t0)\n",
    "    print \"training accuracy:\", clf.score(X, Y)\n",
    " \n",
    "    quiz = pd.read_csv(wd + 'all.csv').astype(np.float32)\n",
    "    quiz = quiz.drop('geoid',axis=1)\n",
    "    quiz = quiz.replace(np.inf, 0)\n",
    "\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    Xtest = quiz[quiz.columns.difference(['t10walk'])]\n",
    "\n",
    "    prediction = clf.predict(Xtest) # do prediction and save it\n",
    "\n",
    "    print 'cross validating...'\n",
    "    scores = cross_val_score(clf, X, Y, cv=5)\n",
    "    print scores\n",
    "    \n",
    "    ftime = str(datetime.now()).replace(' ','-').replace(':','-').replace('.','-')\n",
    "    print ftime\n",
    " \n",
    "    ouCSV = wd+'output/myoutput-'+ftime+'.csv'\n",
    "\n",
    "    with open(ouCSV, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        the_id = 1\n",
    "        for p in prediction:\n",
    "            f.write(\"%s,%s\\n\" % (the_id, p))\n",
    "            the_id += 1\n",
    " \n",
    "    importance = pd.DataFrame(zip(data.columns.difference(['t10walk']), clf.feature_importances_) )\n",
    "    importance.columns = ['feature', 'importance']\n",
    "    importance = importance.sort_values('importance',ascending=False)\n",
    "    importance.to_csv(wd+'output/myoutput-'+ftime+'_importances.csv', index=False)  \n",
    "    \n",
    "    features_names = feature_names[1:]\n",
    "\n",
    "    tree.export_graphviz(clf, out_file='tree.dot', feature_names=feature_names[1:], filled=True, leaves_parallel=True)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rejoin with input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(wd + 'all_w_label.csv')\n",
    "\n",
    "pred = pd.read_csv(wd + 'output/myoutput-2017-09-11-13-21-54-361501.csv')\n",
    "geog = pd.read_csv(INPUT_FOLDER + 'beh_nyc_walkability.csv')\n",
    "\n",
    "df['index_col'] = df.index\n",
    "pred['index_col'] = pred.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['geoid','index_col','t10walk']]\n",
    "\n",
    "df = df.merge(pred, on='index_col', how='left').merge(geog, on='geoid', how='left')\n",
    "\n",
    "df.to_csv(OUTPUT_FOLDER + 'beh_nyc_walkability_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Prediction joined to Walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid</th>\n",
       "      <th>index_col</th>\n",
       "      <th>t10walk_x</th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>t10km2</th>\n",
       "      <th>t10lndkm2</th>\n",
       "      <th>t10cnt</th>\n",
       "      <th>t10resdn1</th>\n",
       "      <th>t10intden</th>\n",
       "      <th>t10entrpy</th>\n",
       "      <th>t10rtlfar</th>\n",
       "      <th>t10sub07d</th>\n",
       "      <th>t10walk_y</th>\n",
       "      <th>t10walkc</th>\n",
       "      <th>the_geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36005000100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.691295</td>\n",
       "      <td>1.690813</td>\n",
       "      <td>25</td>\n",
       "      <td>-4.5421</td>\n",
       "      <td>-2.9774</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.2935</td>\n",
       "      <td>-0.6993</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0106000020E610000001000000010300000001000000F9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36005000200</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.9096</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.9096</td>\n",
       "      <td>0.476454</td>\n",
       "      <td>0.473260</td>\n",
       "      <td>46</td>\n",
       "      <td>-0.0690</td>\n",
       "      <td>-0.4720</td>\n",
       "      <td>-1.3325</td>\n",
       "      <td>-0.3367</td>\n",
       "      <td>-0.6993</td>\n",
       "      <td>-2.9096</td>\n",
       "      <td>1</td>\n",
       "      <td>0106000020E6100000010000000103000000010000009D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36005000400</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.7511</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.7511</td>\n",
       "      <td>0.799501</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.1032</td>\n",
       "      <td>-0.5378</td>\n",
       "      <td>-1.1057</td>\n",
       "      <td>-0.3050</td>\n",
       "      <td>-0.6993</td>\n",
       "      <td>-2.7511</td>\n",
       "      <td>1</td>\n",
       "      <td>0106000020E61000000200000001030000000100000020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36005001600</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.2716</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.2716</td>\n",
       "      <td>0.485080</td>\n",
       "      <td>0.485080</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>-1.3235</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>-0.6993</td>\n",
       "      <td>-2.2716</td>\n",
       "      <td>2</td>\n",
       "      <td>0106000020E6100000010000000103000000010000001C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36005001900</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2115</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2115</td>\n",
       "      <td>1.692264</td>\n",
       "      <td>1.692264</td>\n",
       "      <td>92</td>\n",
       "      <td>0.3584</td>\n",
       "      <td>-0.8116</td>\n",
       "      <td>0.2951</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.2115</td>\n",
       "      <td>3</td>\n",
       "      <td>0106000020E6100000030000000103000000010000000C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         geoid  index_col  t10walk_x  id  prediction    t10km2  t10lndkm2  \\\n",
       "0  36005000100          0     0.0000   1      0.0000  1.691295   1.690813   \n",
       "1  36005000200          1    -2.9096   2     -2.9096  0.476454   0.473260   \n",
       "2  36005000400          2    -2.7511   3     -2.7511  0.799501   0.799500   \n",
       "3  36005001600          3    -2.2716   4     -2.2716  0.485080   0.485080   \n",
       "4  36005001900          4     0.2115   5      0.2115  1.692264   1.692264   \n",
       "\n",
       "   t10cnt  t10resdn1  t10intden  t10entrpy  t10rtlfar  t10sub07d  t10walk_y  \\\n",
       "0      25    -4.5421    -2.9774     0.0000    -2.2935    -0.6993     0.0000   \n",
       "1      46    -0.0690    -0.4720    -1.3325    -0.3367    -0.6993    -2.9096   \n",
       "2      37    -0.1032    -0.5378    -1.1057    -0.3050    -0.6993    -2.7511   \n",
       "3      20     0.1547    -1.3235     0.2939    -0.6975    -0.6993    -2.2716   \n",
       "4      92     0.3584    -0.8116     0.2951    -0.0924     0.3400     0.2115   \n",
       "\n",
       "   t10walkc                                           the_geom  \n",
       "0         0  0106000020E610000001000000010300000001000000F9...  \n",
       "1         1  0106000020E6100000010000000103000000010000009D...  \n",
       "2         1  0106000020E61000000200000001030000000100000020...  \n",
       "3         2  0106000020E6100000010000000103000000010000001C...  \n",
       "4         3  0106000020E6100000030000000103000000010000000C...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Map of Predicted Walkability and BEH Walkability 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"700\"\n",
       "            src=\"https://dms2203.carto.com/builder/e18a2cd4-0edb-412b-9b40-36b08216fe5f/embed?state=%7B%22map%22%3A%7B%22ne%22%3A%5B40.34445080136368%2C-74.66445922851564%5D%2C%22sw%22%3A%5B41.05760862509861%2C-73.35433959960939%5D%2C%22center%22%3A%5B40.702244436175114%2C-74.01008605957033%5D%2C%22zoom%22%3A10%7D%2C%22widgets%22%3A%7B%2257f9458a-b853-444f-a937-0c2bbb5738a6%22%3A%7B%22autoStyle%22%3Atrue%2C%22normalized%22%3Atrue%7D%2C%2210db2f20-bdb2-42dd-a518-cfbb0a34c211%22%3A%7B%22normalized%22%3Atrue%7D%7D%7D\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x14f662850>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://dms2203.carto.com/builder/e18a2cd4-0edb-412b-9b40-36b08216fe5f/embed?state=%7B%22map%22%3A%7B%22ne%22%3A%5B40.34445080136368%2C-74.66445922851564%5D%2C%22sw%22%3A%5B41.05760862509861%2C-73.35433959960939%5D%2C%22center%22%3A%5B40.702244436175114%2C-74.01008605957033%5D%2C%22zoom%22%3A10%7D%2C%22widgets%22%3A%7B%2257f9458a-b853-444f-a937-0c2bbb5738a6%22%3A%7B%22autoStyle%22%3Atrue%2C%22normalized%22%3Atrue%7D%2C%2210db2f20-bdb2-42dd-a518-cfbb0a34c211%22%3A%7B%22normalized%22%3Atrue%7D%7D%7D', width='100%', height=700)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
