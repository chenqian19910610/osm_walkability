{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Import the Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from _settings import * #this has some sensitive stuff like api keys and usernames for carto\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create the data folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pathList = ['input','output','processing','machine_learning','machine_learning/output']\n",
    "\n",
    "dataDir = 'data_v2/'\n",
    "\n",
    "def makeDirectory(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "for i in pathList:\n",
    "    makeDirectory(dataDir + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielmsheehan/GitHub/osm_walkability\n"
     ]
    }
   ],
   "source": [
    "SAVE_FOLDER = dataDir\n",
    "INPUT_FOLDER = SAVE_FOLDER + 'input/'\n",
    "PROCESSING_FOLDER = SAVE_FOLDER + 'processing/'\n",
    "ML_FOLDER = SAVE_FOLDER + 'machine_learning/'\n",
    "curDir = os.getcwd()\n",
    "print curDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stop here if just trying to import modules and set the directories. \n",
    "\n",
    "## Download the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://dms2203.cartodb.com/api/v2/sql?filename=beh_walkability_ct2010.csv&format=csv&q=SELECT geoid,t10km2,t10lndkm2,t10cnt,t10resdn1,t10intden,t10entrpy,t10rtlfar,t10sub07d,t10walk,t10walkc,the_geom FROM ct10'\n",
    "\n",
    "dl_file = urllib.URLopener()\n",
    "dl_file.retrieve(url, INPUT_FOLDER + 'beh_nyc_walkability.csv')\n",
    "\n",
    "url = 'https://s3.amazonaws.com/metro-extracts.mapzen.com/new-york_new-york.osm.pbf'\n",
    "\n",
    "dl_file = urllib.URLopener()\n",
    "dl_file.retrieve(url, INPUT_FOLDER + 'new-york_new-york.osm.pbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paste GDAL/OGR commands into terminal\n",
    "Currently not working using os.system in this virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogr2ogr -f \"SQLite\" -dsco SPATIALITE=YES data_v2/input/new-york_new-york.db data_v2/input/new-york_new-york.osm.pbf\n"
     ]
    }
   ],
   "source": [
    "thecmd = 'ogr2ogr -f \"SQLite\" -dsco SPATIALITE=YES ' + dataDir + 'input/new-york_new-york.db ' + dataDir + 'input/new-york_new-york.osm.pbf'\n",
    "print thecmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogr2ogr -f \"CSV\" data_v2/input/new-york_new-york_points.csv data_v2/input/new-york_new-york.db -lco GEOMETRY=AS_XY -progress -explodecollections -sql \"select * from points\"\n"
     ]
    }
   ],
   "source": [
    "thecmd = 'ogr2ogr -f \"CSV\" ' + dataDir + 'input/new-york_new-york_points.csv ' + dataDir + 'input/new-york_new-york.db -lco GEOMETRY=AS_XY -progress -explodecollections -sql \"select * from points\"'\n",
    "print thecmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean up the OSM data\n",
    "So its just **osm_id** and **latitude, longitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inCSV = dataDir+'input/new-york_new-york_points.csv'\n",
    "df = pd.read_csv(inCSV)\n",
    "df = df[['X','Y','osm_id']]\n",
    "df.columns = ['longitude','latitude','osm_id'] #just delcare col names here\n",
    "df.to_csv(dataDir+'processing/new-york_new-york_points.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Upload OSM point data to Carto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'processing/new-york_new-york_points.csv \"https://'+USERNAME+'.carto.com/api/v1/imports/?api_key=\"'+APIKEY\n",
    "os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the BEH Walkability data to Carto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'input/beh_nyc_walkability.csv \"https://'+USERNAME+'.carto.com/api/v1/imports/?api_key=\"'+APIKEY\n",
    "os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run an intersection to via the Carto Batch SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))'\n",
    "thecmd = '''curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "  \"query\": \"'''+query+'''\"\n",
    "}' \"http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcmd = thecmd\n",
    "result = subprocess.check_output(batcmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user\":\"dms2203\",\"query\":\"SELECT cdb_cartodbfytable(new_york_new_york_points_int_ct10_v2)\",\"job_id\":\"43d3b877-3ba0-4273-829b-beb1cf5af9a3\",\"created_at\":\"2017-09-07T21:22:59.866Z\",\"updated_at\":\"2017-09-07T21:22:59.866Z\",\"status\":\"pending\"}\n",
      "curl -X POST -H \"Content-Type: application/json\" -d '{\n",
      "  \"query\": \"SELECT cdb_cartodbfytable('new_york_new_york_points_int_ct10_v2')\"\n",
      "}' \"http://dms2203.carto.com/api/v2/sql/job?api_key=eb4e90135e42f41bda36e58b7e9c1a660acd98d1\"\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartoDBfy the resulting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT cdb_cartodbfytable('new_york_new_york_points_int_ct10')\"\n",
    "thecmd = '''curl -X POST -H \"Content-Type: application/json\" -d '{\n",
    "  \"query\": \"'''+query+'''\"\n",
    "}' \"http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcmd = thecmd\n",
    "result = subprocess.check_output(batcmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user\":\"dms2203\",\"query\":\"SELECT cdb_cartodbfytable(new_york_new_york_points_int_ct10_v2)\",\"job_id\":\"43d3b877-3ba0-4273-829b-beb1cf5af9a3\",\"created_at\":\"2017-09-07T21:22:59.866Z\",\"updated_at\":\"2017-09-07T21:22:59.866Z\",\"status\":\"pending\"}\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or run your SQL via the [Carto Batch API](https://cartodb.github.io/customer_success/batch/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the OSM - BEH Walkability Census Tracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data_v2/processing//new_york_new_york_points_int_ct10.csv',\n",
       " <httplib.HTTPMessage instance at 0x110e2acf8>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://dms2203.cartodb.com/api/v2/sql?format=csv&q=SELECT geoid, osm_id, latitude, longitude FROM new_york_new_york_points_int_ct10&api_key='+APIKEY\n",
    "\n",
    "dl_file = urllib.URLopener()\n",
    "dl_file.retrieve(url, PROCESSING_FOLDER+'/new_york_new_york_points_int_ct10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up OSM Highway & Tags Other into seperate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCSV = INPUT_FOLDER+'/new-york_new-york_points.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "dfH = df[['osm_id','highway']]\n",
    "dfT = df[['osm_id','other_tags']]\n",
    "\n",
    "dfH = dfH.dropna(subset=['highway'])\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/osm_highway.csv'\n",
    "dfH.to_csv(ouCSV, index=False)\n",
    "\n",
    "dfT = dfT.dropna(subset=['other_tags'])\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/osm_other_tags.csv'\n",
    "dfT.to_csv(ouCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "inCSV = PROCESSING_FOLDER+'/osm_highway.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "df['count'] = 1\n",
    "\n",
    "dfp = pd.pivot_table(df, values='count', index='osm_id',columns='highway', aggfunc=np.sum)\n",
    "\n",
    "dfp = dfp.rename(columns=lambda x: x.replace(';', '_'))\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/osm_highway_pivot.csv'\n",
    "\n",
    "dfp.to_csv(ouCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Tags Other \n",
    "This took a very long long time to run. I should look into rewriting the code, maybe not store all these dataframes into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...concatinating...\n"
     ]
    }
   ],
   "source": [
    "inCSV = PROCESSING_FOLDER + '/osm_other_tags.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "tagList = df.values.tolist()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for h in tagList:\n",
    "    bucket = []\n",
    "    x = h[1].split('\",\"')\n",
    "    \n",
    "    for i in x:\n",
    "        z = i.split(\"=>\")\n",
    "        osm = []\n",
    "        \n",
    "        for j in z: \n",
    "            osm.append(j.replace('\"','').replace(':','_'))\n",
    "        bucket.append(osm)\n",
    "\n",
    "    df = pd.DataFrame(data=[h[0]], columns=['osm_id'])\n",
    "    \n",
    "    for i in bucket:\n",
    "        df[i[0]] = i[1]\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "print '...concatenating...'\n",
    "df = pd.concat(df_list)\n",
    "df = df.set_index('osm_id')\n",
    "\n",
    "df.to_csv(PROCESSING_FOLDER + '/osm_tags_pivot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Other Tags reduce low features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addr_city 0.968342437816 363\n",
      "addr_postcode 0.324439387292 889\n",
      "addr_state 0.971963337785 16\n",
      "amenity 0.880495836688 122\n",
      "capacity 0.975261201406 88\n",
      "cityracks.large 0.977893706577 9\n",
      "cityracks.small 0.977893706577 9\n",
      "cityracks.street 0.977893706577 837\n",
      "crossing 0.974528342824 10\n",
      "cuisine 0.985419971361 374\n",
      "gnis_county_id 0.958183667861 29\n",
      "gnis_created 0.956901165342 169\n",
      "gnis_state_id 0.958183667861 3\n",
      "leisure 0.988780513676 35\n",
      "natural 0.966071540498 15\n",
      "operator 0.988370691442 404\n",
      "power 0.953612944597 10\n",
      "railway 0.968294223435 21\n",
      "religion 0.986027472554 9\n",
      "shop 0.977127097928 183\n"
     ]
    }
   ],
   "source": [
    "inCSV = PROCESSING_FOLDER + '/osm_tags_pivot.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "dfLen = len(df.index)\n",
    "\n",
    "keeperCols = []\n",
    "\n",
    "for i in df.columns:\n",
    "    countNulls = df[i].isnull().sum()\n",
    "    pctNull = countNulls/(dfLen * 1.0)\n",
    "    if df[i].nunique() < 1000:\n",
    "        if pctNull < 0.99:\n",
    "            print i, pctNull, df[i].nunique()\n",
    "            keeperCols.append(i)\n",
    "\n",
    "keeperCols     \n",
    "    \n",
    "df = df[keeperCols+['osm_id']]        \n",
    "\n",
    "ouCSV = PROCESSING_FOLDER + '/osm_tags_pivot_cln.csv'\n",
    "\n",
    "df.to_csv(ouCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data, One Hot Encode, Normalize variables to Count by Square Kilometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTra = INPUT_FOLDER + '/beh_nyc_walkability.csv'\n",
    "inInt = PROCESSING_FOLDER + '/new_york_new_york_points_int_ct10.csv'\n",
    "inHig = PROCESSING_FOLDER + '/osm_highway_pivot.csv'\n",
    "inTag = PROCESSING_FOLDER + '/osm_tags_pivot_cln.csv'\n",
    "\n",
    "dfInt = pd.read_csv(inInt)\n",
    "dfTra = pd.read_csv(inTra)\n",
    "dfHig = pd.read_csv(inHig)\n",
    "dfTag = pd.read_csv(inTag)\n",
    "\n",
    "dfInt = dfInt[['geoid','osm_id']]\n",
    "dfTra = dfTra[['geoid','t10walk','t10lndkm2']]\n",
    "\n",
    "df = dfInt.merge(dfTra, how='left', on='geoid')\n",
    "df = df.merge(dfHig, how='left', on='osm_id')\n",
    "df = df.merge(dfTag, how='left', on='osm_id')\n",
    "\n",
    "df = df.drop(['osm_id','gnis_created','gnis_county_id','gnis_state_id','capacity','addr_state','addr_city','religion'], axis=1)\n",
    "#df = df.drop(['osm_id'], axis=1)\n",
    "\n",
    "dfStringCols = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object': #select all non numerical columns\n",
    "        dfStringCols.append(i)\n",
    "\n",
    "df['index_col'] = df.index\n",
    "\n",
    "colTotal = 0\n",
    "for i in dfStringCols:\n",
    "    one_hot = pd.get_dummies(df[i])\n",
    "    colTotal = colTotal + len(one_hot.columns)\n",
    "    one_hot = one_hot.rename(columns = lambda x : i + '_one_hot_' + x)\n",
    "    one_hot['index_col'] = one_hot.index\n",
    "    df = df.merge(one_hot, on='index_col', how='left')\n",
    "\n",
    "df = df.drop(dfStringCols, axis=1)\n",
    "df = df.groupby(['geoid','t10walk','t10lndkm2'],as_index=False).sum() # sum osm features to tract boundaries\n",
    "df = df.fillna(0)\n",
    "\n",
    "colsList = df.columns\n",
    "\n",
    "for i in colsList:\n",
    "    if i != 'geoid' and i != 'osm_id'  and i != 't10walk'  and i != 't10lndkm2':\n",
    "        df[i] = df[i]/df['t10lndkm2'] # get osm feature density by tract area\n",
    "\n",
    "df = df.drop(['t10lndkm2','index_col'], axis=1)\n",
    "df = df.fillna(0)\n",
    "\n",
    "ouCSV = PROCESSING_FOLDER+'/walkability_highway_tags_features.csv'\n",
    "df.to_csv(ouCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2163\n",
      "1725\n",
      "438\n"
     ]
    }
   ],
   "source": [
    "inCSV = PROCESSING_FOLDER + '/walkability_highway_tags_features.csv'\n",
    "\n",
    "df = pd.read_csv(inCSV)\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8 #split 80/20, 80% for training, 20% for testing - http://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "print len(df)\n",
    "print len(train)\n",
    "print len(test)\n",
    "\n",
    "test.to_csv(ML_FOLDER + '/test_w_label.csv', index=False)\n",
    "\n",
    "testDropCols = ['t10walk'] #['health','label']\n",
    "\n",
    "test = test.drop(testDropCols,axis=1)\n",
    "\n",
    "train.to_csv(ML_FOLDER + '/train.csv',index=False)\n",
    "test.to_csv(ML_FOLDER + '/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-08 03:55:23.119308\n",
      "2017-09-08 03:55:24.304313\n",
      "training time: 0:00:00.081082\n",
      "training accuracy: 0.946702578361\n",
      "cross validating...\n",
      "[-0.12658276 -2.27219534  0.09970833 -1.46759867 -0.39592374]\n",
      "2017-09-08-03-55-24-936718\n",
      "Index([u't10walk', u'bus_stop', u'bus_stop_traffic_signals', u'crossing_x',\n",
      "       u'elevator', u'emergency_access_point', u'footway', u'give_way',\n",
      "       u'mini_roundabout', u'motorway_junction',\n",
      "       ...\n",
      "       u'shop_one_hot_shoes', u'shop_one_hot_sports',\n",
      "       u'shop_one_hot_stationery', u'shop_one_hot_storage_units',\n",
      "       u'shop_one_hot_supermarket', u'shop_one_hot_tax', u'shop_one_hot_toys',\n",
      "       u'shop_one_hot_vacant', u'shop_one_hot_video_games',\n",
      "       u'shop_one_hot_yes'],\n",
      "      dtype='object', length=1336)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "print datetime.now()\n",
    " \n",
    "wd = ML_FOLDER + '/'\n",
    " \n",
    "def main():\n",
    "    data = pd.read_csv(wd + 'train.csv').astype(np.float32)\n",
    "    data = data.replace(np.inf, 0)\n",
    "    data = data.drop('geoid',axis=1)\n",
    "    data = data.fillna(data.mean())\n",
    "    feature_names = data.columns\n",
    "    \n",
    "    t0 = datetime.now()\n",
    "\n",
    "    X = data[data.columns.difference(['t10walk'])]\n",
    "    Y = data['t10walk']\n",
    "\n",
    "    clf = tree.DecisionTreeRegressor()\n",
    "    #clf = ExtraTreeRegressor()\n",
    "\n",
    "    print datetime.now()\n",
    " \n",
    "    t0 = datetime.now()\n",
    "    clf.fit(X, Y)\n",
    "    print \"training time:\", (datetime.now() - t0)\n",
    "    print \"training accuracy:\", clf.score(X, Y)\n",
    " \n",
    "    quiz = pd.read_csv(wd + 'test.csv').astype(np.float32)\n",
    "    quiz = quiz.drop('geoid',axis=1)\n",
    "    quiz = quiz.replace(np.inf, 0)\n",
    "\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    Xtest = quiz[quiz.columns.difference(['t10walk'])]\n",
    "\n",
    "    prediction = clf.predict(Xtest) # do prediction and save it\n",
    "\n",
    "    print 'cross validating...'\n",
    "    scores = cross_val_score(clf, X, Y, cv=5)\n",
    "    print scores\n",
    "    \n",
    "    ftime = str(datetime.now()).replace(' ','-').replace(':','-').replace('.','-')\n",
    "    print ftime\n",
    " \n",
    "    ouCSV = wd+'output/myoutput-'+ftime+'.csv'\n",
    "\n",
    "    with open(ouCSV, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        the_id = 1\n",
    "        for p in prediction:\n",
    "            f.write(\"%s,%s\\n\" % (the_id, p))\n",
    "            the_id += 1\n",
    " \n",
    "    importance = pd.DataFrame(zip(data.columns.difference(['t10walk']), clf.feature_importances_) )\n",
    "    importance.columns = ['feature', 'importance']\n",
    "    importance = importance.sort_values('importance',ascending=False)\n",
    "    importance.to_csv(wd+'output/myoutput-'+ftime+'_importances.csv', index=False)  \n",
    "    \n",
    "    features_names = feature_names[1:]\n",
    "\n",
    "    tree.export_graphviz(clf, out_file='tree.dot', feature_names=feature_names[1:], filled=True, leaves_parallel=True)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
